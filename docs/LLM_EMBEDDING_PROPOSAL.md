# LLM Embedding å¢å¼ºæ–°é—»æ¨èç³»ç»Ÿ - æŠ€æœ¯æ–¹æ¡ˆ

## 1. æ–¹æ¡ˆæ¦‚è¿°

åœ¨ç°æœ‰çš„ **GNN + KG** æ¨èç³»ç»ŸåŸºç¡€ä¸Šï¼Œæ¥å…¥ **LLM Embedding API**ï¼Œåˆ©ç”¨å¤§æ¨¡å‹çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡æ–°é—»å‘é‡ï¼Œæå‡æ¨èæ•ˆæœã€‚

### 1.1 æ ¸å¿ƒæ€æƒ³

```
åŸæ¶æ„: æ–°é—»è¡¨ç¤º = IDåµŒå…¥ + GNNå®ä½“åµŒå…¥

å‡çº§å: æ–°é—»è¡¨ç¤º = IDåµŒå…¥ + GNNå®ä½“åµŒå…¥ + LLMæ–‡æœ¬åµŒå…¥
                         â†“
                     å¤šæ¨¡æ€èåˆå±‚
                         â†“
                  å¢å¼ºçš„æ–°é—»è¡¨ç¤º
```

### 1.2 é¢„æœŸæ”¶ç›Š

âœ… **æ›´å¥½çš„è¯­ä¹‰ç†è§£**: LLM èƒ½æ•æ‰æ ‡é¢˜/æ‘˜è¦çš„æ·±å±‚è¯­ä¹‰
âœ… **å†·å¯åŠ¨èƒ½åŠ›**: æ–°æ–°é—»å³ä½¿æ²¡æœ‰ ID åµŒå…¥ï¼Œä¹Ÿèƒ½æœ‰é«˜è´¨é‡è¡¨ç¤º
âœ… **è·¨æ–°é—»æ³›åŒ–**: ç›¸ä¼¼ä¸»é¢˜çš„æ–°é—»åœ¨å‘é‡ç©ºé—´ä¸­æ›´æ¥è¿‘
âœ… **å¤šè¯­è¨€æ”¯æŒ**: LLM æ”¯æŒå¤šè¯­è¨€æ–‡æœ¬ï¼ˆå¦‚éœ€è¦ï¼‰

---

## 2. æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å‡†å¤‡é˜¶æ®µ (ç¦»çº¿)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ–°é—»æ•°æ® (news.tsv)
  â”œâ”€ æ ‡é¢˜ (title)
  â”œâ”€ æ‘˜è¦ (abstract)
  â””â”€ å®ä½“ (entities)
      â†“
  [æ–‡æœ¬æ‹¼æ¥]
      â†“
  "Title: {title}\nAbstract: {abstract}"
      â†“
  [æ‰¹é‡è°ƒç”¨ LLM Embedding API]
      â†“
  LLM Embeddings (51K Ã— 1536ç»´)
      â†“
  [ä¿å­˜åˆ°æ–‡ä»¶]
      â†“
  news_llm_embeddings.npy


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è®­ç»ƒ/æ¨ç†é˜¶æ®µ (åœ¨çº¿)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥: æ–°é—» ID
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ID Embeddingâ”‚ LLM Embedding â”‚  GNN Embeddingâ”‚
â”‚   (128ç»´)     â”‚  (1536ç»´)     â”‚   (128ç»´)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“              â†“              â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
            [å¤šæ¨¡æ€èåˆå±‚ - Attention Gate]
                        â†“
            æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦ (256ç»´)
                        â†“
            æœ€ç»ˆæ–°é—»è¡¨ç¤º (256ç»´)
                        â†“
            ä¸ç”¨æˆ·è¡¨ç¤ºè®¡ç®—ç›¸ä¼¼åº¦
                        â†“
              ç‚¹å‡»æ¦‚ç‡é¢„æµ‹
```

### 2.2 æ¨¡å‹æ¶æ„ç»†èŠ‚

```python
class MultiModalNewsEncoder(nn.Module):
    """
    å¤šæ¨¡æ€æ–°é—»ç¼–ç å™¨
    èåˆ: IDåµŒå…¥ + LLMæ–‡æœ¬åµŒå…¥ + GNNå®ä½“åµŒå…¥
    """

    def __init__(
        self,
        num_news,
        id_emb_dim=128,      # ID åµŒå…¥ç»´åº¦
        llm_emb_dim=1536,    # LLM åµŒå…¥ç»´åº¦ (OpenAI)
        gnn_emb_dim=128,     # GNN åµŒå…¥ç»´åº¦
        output_dim=256,      # è¾“å‡ºç»´åº¦
        use_llm=True,        # æ˜¯å¦ä½¿ç”¨ LLM
        use_gnn=True,        # æ˜¯å¦ä½¿ç”¨ GNN
        fusion_method='attention'  # èåˆæ–¹æ³•
    ):
        super().__init__()

        # 1. ID åµŒå…¥
        self.id_embedding = nn.Embedding(num_news, id_emb_dim)

        # 2. LLM åµŒå…¥æŠ•å½±å±‚
        if use_llm:
            self.llm_proj = nn.Sequential(
                nn.Linear(llm_emb_dim, output_dim),
                nn.LayerNorm(output_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            )

        # 3. GNN (å·²æœ‰)
        if use_gnn:
            self.gnn = NewsEntityGNN(...)
            self.gnn_proj = nn.Sequential(
                nn.Linear(gnn_emb_dim, output_dim),
                nn.LayerNorm(output_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            )

        # 4. ID åµŒå…¥æŠ•å½±å±‚
        self.id_proj = nn.Sequential(
            nn.Linear(id_emb_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # 5. èåˆå±‚
        if fusion_method == 'attention':
            # æ³¨æ„åŠ›é—¨æ§èåˆ
            self.fusion_gate = nn.Sequential(
                nn.Linear(output_dim * 3, 3),
                nn.Softmax(dim=-1)
            )
        elif fusion_method == 'concat':
            # ç®€å•æ‹¼æ¥ + MLP
            self.fusion_mlp = nn.Sequential(
                nn.Linear(output_dim * 3, output_dim * 2),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(output_dim * 2, output_dim)
            )

    def forward(self, news_ids, llm_embeddings=None, gnn_embeddings=None):
        """
        Args:
            news_ids: (batch_size,)
            llm_embeddings: (num_news, llm_emb_dim) é¢„åŠ è½½çš„ LLM åµŒå…¥
            gnn_embeddings: (num_news, gnn_emb_dim) é¢„è®¡ç®—çš„ GNN åµŒå…¥

        Returns:
            news_repr: (batch_size, output_dim)
        """
        # 1. è·å–ä¸‰ç§è¡¨ç¤º
        id_emb = self.id_embedding(news_ids)  # (B, id_emb_dim)
        id_repr = self.id_proj(id_emb)        # (B, output_dim)

        # 2. LLM è¡¨ç¤º
        if llm_embeddings is not None:
            llm_emb = llm_embeddings[news_ids]  # (B, llm_emb_dim)
            llm_repr = self.llm_proj(llm_emb)   # (B, output_dim)
        else:
            llm_repr = torch.zeros_like(id_repr)

        # 3. GNN è¡¨ç¤º
        if gnn_embeddings is not None:
            gnn_emb = gnn_embeddings[news_ids]  # (B, gnn_emb_dim)
            gnn_repr = self.gnn_proj(gnn_emb)   # (B, output_dim)
        else:
            gnn_repr = torch.zeros_like(id_repr)

        # 4. èåˆ
        if self.fusion_method == 'attention':
            # æ³¨æ„åŠ›é—¨æ§
            concat = torch.cat([id_repr, llm_repr, gnn_repr], dim=-1)
            gate = self.fusion_gate(concat)  # (B, 3)

            news_repr = (
                gate[:, 0:1] * id_repr +
                gate[:, 1:2] * llm_repr +
                gate[:, 2:3] * gnn_repr
            )
        else:
            # ç®€å•æ‹¼æ¥
            concat = torch.cat([id_repr, llm_repr, gnn_repr], dim=-1)
            news_repr = self.fusion_mlp(concat)

        return news_repr
```

---

## 3. LLM Embedding API é€‰æ‹©

### 3.1 ä¸»æµ API å¯¹æ¯”

| API æœåŠ¡ | æ¨¡å‹åç§° | ç»´åº¦ | ä»·æ ¼ (USD/1M tokens) | æ€§èƒ½ | æ¨èåº¦ |
|---------|---------|------|---------------------|------|--------|
| **OpenAI** | text-embedding-3-small | 1536 | $0.02 | â­â­â­â­ | âœ… æ¨è |
| **OpenAI** | text-embedding-3-large | 3072 | $0.13 | â­â­â­â­â­ | âš ï¸ è´µ |
| **OpenAI** | text-embedding-ada-002 | 1536 | $0.10 | â­â­â­ | è¢«æ·˜æ±° |
| **æ™ºè°±AI** | embedding-2 | 1024 | Â¥0.0005/åƒtokens | â­â­â­â­ | âœ… å›½å†…é¦–é€‰ |
| **ç™¾åº¦æ–‡å¿ƒ** | embedding-v1 | 384 | Â¥0.002/åƒtokens | â­â­â­ | å¯ç”¨ |
| **é˜¿é‡Œé€šä¹‰** | text-embedding-v2 | 1536 | Â¥0.0007/åƒtokens | â­â­â­â­ | âœ… æ¨è |
| **Cohere** | embed-english-v3.0 | 1024 | $0.10 | â­â­â­â­ | è‹±æ–‡ä¼˜ |

### 3.2 æ¨èé…ç½®

**æ–¹æ¡ˆ 1: OpenAI (å›½é™…)**
```python
from openai import OpenAI

client = OpenAI(api_key="sk-...")

response = client.embeddings.create(
    model="text-embedding-3-small",
    input=["News title and abstract here"],
    encoding_format="float"
)
embedding = response.data[0].embedding  # 1536ç»´
```

**æ–¹æ¡ˆ 2: æ™ºè°±AI (å›½å†…)**
```python
from zhipuai import ZhipuAI

client = ZhipuAI(api_key="...")

response = client.embeddings.create(
    model="embedding-2",
    input="News title and abstract here"
)
embedding = response.data[0].embedding  # 1024ç»´
```

**æ–¹æ¡ˆ 3: é˜¿é‡Œé€šä¹‰ (å›½å†…ï¼Œæ¨è)**
```python
import dashscope

response = dashscope.TextEmbedding.call(
    model=dashscope.TextEmbedding.Models.text_embedding_v2,
    input="News title and abstract here"
)
embedding = response.output['embeddings'][0]['embedding']  # 1536ç»´
```

---

## 4. å®æ–½æ–¹æ¡ˆ

### 4.1 é˜¶æ®µ 1: é¢„è®¡ç®— LLM Embeddings (ç¦»çº¿)

```python
# è„šæœ¬: src/precompute_llm_embeddings.py

import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from openai import OpenAI

def precompute_llm_embeddings(
    news_path,
    output_path,
    api_key,
    model="text-embedding-3-small",
    batch_size=100
):
    """
    é¢„è®¡ç®—æ‰€æœ‰æ–°é—»çš„ LLM embeddings

    Args:
        news_path: news.tsv è·¯å¾„
        output_path: è¾“å‡º .npy æ–‡ä»¶è·¯å¾„
        api_key: LLM API key
        batch_size: æ‰¹é‡è°ƒç”¨å¤§å°
    """
    # 1. åŠ è½½æ–°é—»æ•°æ®
    news_df = pd.read_csv(news_path, sep='\t', ...)

    # 2. æ„å»ºæ–‡æœ¬
    texts = []
    for _, row in news_df.iterrows():
        title = row['title'] if pd.notna(row['title']) else ""
        abstract = row['abstract'] if pd.notna(row['abstract']) else ""

        # æ‹¼æ¥ç­–ç•¥
        text = f"Title: {title}\nAbstract: {abstract}"
        texts.append(text)

    # 3. æ‰¹é‡è°ƒç”¨ API
    client = OpenAI(api_key=api_key)
    embeddings = []

    for i in tqdm(range(0, len(texts), batch_size)):
        batch = texts[i:i+batch_size]

        try:
            response = client.embeddings.create(
                model=model,
                input=batch
            )

            batch_embeddings = [
                data.embedding for data in response.data
            ]
            embeddings.extend(batch_embeddings)

        except Exception as e:
            print(f"Error at batch {i}: {e}")
            # é™çº§ç­–ç•¥ï¼šä½¿ç”¨é›¶å‘é‡
            embeddings.extend([
                [0.0] * 1536 for _ in batch
            ])

    # 4. ä¿å­˜
    embeddings = np.array(embeddings, dtype=np.float32)

    # åˆ›å»ºæ˜ å°„: news_id -> embedding index
    news_id_to_idx = {
        news_id: idx
        for idx, news_id in enumerate(news_df['news_id'])
    }

    np.save(output_path, embeddings)

    # ä¿å­˜æ˜ å°„
    import pickle
    with open(output_path.replace('.npy', '_id_mapping.pkl'), 'wb') as f:
        pickle.dump(news_id_to_idx, f)

    print(f"Saved {len(embeddings)} embeddings to {output_path}")
    print(f"Shape: {embeddings.shape}")

    return embeddings, news_id_to_idx
```

### 4.2 é˜¶æ®µ 2: æ•°æ®åŠ è½½å™¨æ”¹é€ 

```python
# ä¿®æ”¹: src/data_loader.py

class MINDDatasetWithLLM(MINDDataset):
    """æ‰©å±•æ•°æ®é›†ï¼Œæ”¯æŒåŠ è½½ LLM embeddings"""

    def __init__(self, behaviors_path, news_path,
                 llm_embedding_path=None, mode='train'):
        super().__init__(behaviors_path, news_path, mode)

        # åŠ è½½ LLM embeddings
        if llm_embedding_path and os.path.exists(llm_embedding_path):
            self.llm_embeddings = np.load(llm_embedding_path)
            print(f"Loaded LLM embeddings: {self.llm_embeddings.shape}")
        else:
            self.llm_embeddings = None

    def get_llm_embeddings_tensor(self):
        """è¿”å› PyTorch tensor"""
        if self.llm_embeddings is not None:
            return torch.from_numpy(self.llm_embeddings).float()
        return None
```

### 4.3 é˜¶æ®µ 3: æ¨¡å‹æ”¹é€ 

```python
# æ–°æ–‡ä»¶: src/model_llm.py

class LLMEnhancedRecommender(nn.Module):
    """
    LLM + GNN + ID ä¸‰æ¨¡æ€èåˆæ¨èæ¨¡å‹
    """

    def __init__(
        self,
        num_users,
        num_news,
        embedding_dim=128,
        llm_emb_dim=1536,
        gnn_emb_dim=128,
        output_dim=256,
        use_llm=True,
        use_gnn=True,
        dropout=0.2
    ):
        super().__init__()

        # ç”¨æˆ·ç¼–ç å™¨ (å¤ç”¨åŸæœ‰)
        self.user_encoder = UserEncoder(...)

        # æ–°é—»ç¼–ç å™¨ (å¤šæ¨¡æ€)
        self.news_encoder = MultiModalNewsEncoder(
            num_news=num_news,
            id_emb_dim=embedding_dim,
            llm_emb_dim=llm_emb_dim,
            gnn_emb_dim=gnn_emb_dim,
            output_dim=output_dim,
            use_llm=use_llm,
            use_gnn=use_gnn
        )

    def forward(self, user_idx, news_idx, history,
                llm_embeddings=None, gnn_embeddings=None):
        """
        Args:
            llm_embeddings: (num_news, llm_emb_dim) å…¨å±€ LLM åµŒå…¥
            gnn_embeddings: (num_news, gnn_emb_dim) å…¨å±€ GNN åµŒå…¥
        """
        # ç”¨æˆ·è¡¨ç¤º
        user_repr = self.user_encoder(user_idx, history, ...)

        # æ–°é—»è¡¨ç¤º (å¤šæ¨¡æ€èåˆ)
        news_repr = self.news_encoder(
            news_idx,
            llm_embeddings=llm_embeddings,
            gnn_embeddings=gnn_embeddings
        )

        # ç›¸ä¼¼åº¦
        scores = torch.sum(user_repr * news_repr, dim=1)
        scores = torch.sigmoid(scores)

        return scores
```

### 4.4 é˜¶æ®µ 4: è®­ç»ƒè„šæœ¬æ”¹é€ 

```python
# æ–°æ–‡ä»¶: src/train_llm.py

def main(args):
    # 1. åŠ è½½æ•°æ®
    train_dataset = MINDDatasetWithLLM(
        behaviors_path=...,
        news_path=...,
        llm_embedding_path=args.llm_embedding_path
    )

    # 2. è·å– LLM embeddings (ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜/GPU)
    llm_embeddings = train_dataset.get_llm_embeddings_tensor()
    if llm_embeddings is not None:
        llm_embeddings = llm_embeddings.to(device)

    # 3. åˆ›å»ºæ¨¡å‹
    model = LLMEnhancedRecommender(
        num_users=num_users,
        num_news=num_news,
        use_llm=args.use_llm,
        use_gnn=args.use_gnn
    ).to(device)

    # 4. è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        # é¢„è®¡ç®— GNN embeddings (å¦‚æœéœ€è¦)
        if args.use_gnn:
            gnn_embeddings = model.get_gnn_embeddings()
        else:
            gnn_embeddings = None

        # è®­ç»ƒ
        for batch in train_loader:
            scores = model(
                user_idx, news_idx, history,
                llm_embeddings=llm_embeddings,  # å…¨å±€å…±äº«
                gnn_embeddings=gnn_embeddings   # å…¨å±€å…±äº«
            )
            ...
```

---

## 5. æˆæœ¬ä¼°ç®—

### 5.1 MIND-small æ•°æ®é›†

| é¡¹ç›® | æ•°é‡ | è¯¦æƒ… |
|------|------|------|
| è®­ç»ƒé›†æ–°é—» | 51,282 | |
| éªŒè¯é›†æ–°é—» | 42,416 | |
| **æ€»æ–°é—»æ•°** | **~51K** | å»é‡å |
| å¹³å‡æ–‡æœ¬é•¿åº¦ | ~100 tokens | title + abstract |
| **æ€» tokens** | **~5M** | |

### 5.2 API æˆæœ¬

**OpenAI text-embedding-3-small**
- ä»·æ ¼: $0.02 / 1M tokens
- æ€»æˆæœ¬: 5M Ã— $0.02 = **$0.10** (çº¦ Â¥0.7)

**æ™ºè°±AI embedding-2**
- ä»·æ ¼: Â¥0.0005 / 1K tokens
- æ€»æˆæœ¬: 5M Ã— Â¥0.0005 = **Â¥2.5**

**é˜¿é‡Œé€šä¹‰ text-embedding-v2**
- ä»·æ ¼: Â¥0.0007 / 1K tokens
- æ€»æˆæœ¬: 5M Ã— Â¥0.0007 = **Â¥3.5**

**ç»“è®º**: æˆæœ¬æä½ï¼Œä¸€æ¬¡æ€§æŠ•å…¥å¯å¿½ç•¥ä¸è®¡ã€‚

---

## 6. ä¼˜åŒ–ç­–ç•¥

### 6.1 æ‰¹é‡è°ƒç”¨ä¼˜åŒ–

```python
# æ‰¹é‡å¤§å°å»ºè®®
batch_size = 100  # OpenAI æ”¯æŒæœ€å¤š 2048 ä¸ªè¾“å…¥

# å¹¶å‘è°ƒç”¨ (è°¨æ…ä½¿ç”¨ï¼Œé¿å…è§¦å‘é™æµ)
import asyncio
from openai import AsyncOpenAI

async def batch_embed(texts, batch_size=100):
    client = AsyncOpenAI(api_key=...)

    tasks = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        task = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        tasks.append(task)

    responses = await asyncio.gather(*tasks)
    return responses
```

### 6.2 ç¼“å­˜ç­–ç•¥

```python
import hashlib
import pickle
from pathlib import Path

class EmbeddingCache:
    """LLM Embedding ç¼“å­˜"""

    def __init__(self, cache_dir="./cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def _get_key(self, text, model):
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{model}:{text}"
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, text, model):
        """è·å–ç¼“å­˜"""
        key = self._get_key(text, model)
        cache_file = self.cache_dir / f"{key}.pkl"

        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def set(self, text, model, embedding):
        """è®¾ç½®ç¼“å­˜"""
        key = self._get_key(text, model)
        cache_file = self.cache_dir / f"{key}.pkl"

        with open(cache_file, 'wb') as f:
            pickle.dump(embedding, f)
```

### 6.3 é”™è¯¯å¤„ç†

```python
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def call_embedding_api(texts, model="text-embedding-3-small"):
    """å¸¦é‡è¯•çš„ API è°ƒç”¨"""
    try:
        response = client.embeddings.create(
            model=model,
            input=texts
        )
        return [data.embedding for data in response.data]

    except Exception as e:
        print(f"API Error: {e}")
        raise
```

---

## 7. å®éªŒå¯¹æ¯”æ–¹æ¡ˆ

### 7.1 å¯¹æ¯”å®éªŒè®¾è®¡

| æ¨¡å‹ | IDåµŒå…¥ | GNN | LLM | é¢„æœŸæ•ˆæœ |
|------|--------|-----|-----|---------|
| Baseline | âœ… | âŒ | âŒ | åŸºå‡† |
| +GNN | âœ… | âœ… | âŒ | +5% AUC |
| +LLM | âœ… | âŒ | âœ… | +10% AUC |
| **+GNN+LLM** | âœ… | âœ… | âœ… | **+15% AUC** |

### 7.2 è¯„ä¼°æŒ‡æ ‡

```python
# ä¸»è¦æŒ‡æ ‡
- AUC (Area Under Curve)
- MRR (Mean Reciprocal Rank)
- nDCG@5, nDCG@10
- Hit Rate@10

# é¢å¤–åˆ†æ
- å†·å¯åŠ¨æ€§èƒ½ (æ–°æ–°é—» Top-K)
- é•¿å°¾æ–°é—»è¦†ç›–ç‡
- ä¸åŒç±»åˆ«æ–°é—»çš„æ€§èƒ½
```

---

## 8. å®æ–½æ—¶é—´çº¿

| é˜¶æ®µ | ä»»åŠ¡ | æ—¶é—´ | è´Ÿè´£äºº |
|------|------|------|--------|
| **Week 1** | é¢„è®¡ç®— LLM embeddings | 1å¤© | æ•°æ®å·¥ç¨‹å¸ˆ |
| | æ•°æ®åŠ è½½å™¨æ”¹é€  | 1å¤© | |
| | æ¨¡å‹æ¶æ„è®¾è®¡ | 2å¤© | ç®—æ³•å·¥ç¨‹å¸ˆ |
| **Week 2** | è®­ç»ƒè„šæœ¬æ”¹é€  | 2å¤© | |
| | åŸºç¡€å®éªŒ (Baseline vs +LLM) | 2å¤© | |
| **Week 3** | å®Œæ•´å®éªŒ (+GNN+LLM) | 2å¤© | |
| | è¶…å‚æ•°è°ƒä¼˜ | 2å¤© | |
| **Week 4** | æ•ˆæœè¯„ä¼°å’ŒæŠ¥å‘Š | 2å¤© | |
| | éƒ¨ç½²å’Œä¸Šçº¿ | 2å¤© | |

---

## 9. é£é™©ä¸ç¼“è§£

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|---------|
| API é™æµ | é¢„è®¡ç®—å¤±è´¥ | æ‰¹é‡è°ƒç”¨ + é‡è¯• + ç¼“å­˜ |
| æˆæœ¬è¶…é¢„ç®— | é¡¹ç›®ä¸­æ­¢ | å…ˆåœ¨ tiny æ•°æ®é›†æµ‹è¯• |
| æ•ˆæœä¸ä½³ | æµªè´¹æ—¶é—´ | å¿«é€Ÿå®éªŒï¼Œè®¾å®šæ—©åœæ¡ä»¶ |
| ç»´åº¦ä¸åŒ¹é… | æ¨¡å‹è®­ç»ƒå¤±è´¥ | æŠ•å½±å±‚å¯¹é½ç»´åº¦ |
| å†…å­˜ä¸è¶³ | è®­ç»ƒå´©æºƒ | ä½¿ç”¨ float16 + æ¢¯åº¦ç´¯ç§¯ |

---

## 10. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### 10.1 ç«‹å³å¼€å§‹

1. âœ… é€‰æ‹© LLM API (æ¨è: é˜¿é‡Œé€šä¹‰ / OpenAI)
2. âœ… åœ¨ tiny æ•°æ®é›†ä¸Šé¢„è®¡ç®— embeddings (500 æ¡æ–°é—»)
3. âœ… å®ç°ç®€å•èåˆæ¨¡å‹ (Concat æ–¹æ³•)
4. âœ… è¿è¡ŒåŸºç¡€å®éªŒï¼ŒéªŒè¯å¯è¡Œæ€§

### 10.2 å¿«é€ŸéªŒè¯è„šæœ¬

```bash
# Step 1: é¢„è®¡ç®— (tiny æ•°æ®é›†)
python src/precompute_llm_embeddings.py \
    --news_path data/mind_tiny/news.tsv \
    --output_path data/mind_tiny/llm_embeddings.npy \
    --api_key YOUR_API_KEY \
    --model text-embedding-3-small

# Step 2: è®­ç»ƒ
python src/train_llm.py \
    --epochs 3 \
    --use_llm \
    --use_gnn \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy

# Step 3: å¯¹æ¯”
python compare_results.py \
    --baseline output/baseline/eval/metrics.json \
    --gnn output/gnn/eval/metrics.json \
    --llm output/llm/eval/metrics.json \
    --gnn_llm output/gnn_llm/eval/metrics.json
```

---

## 11. å‚è€ƒæ–‡çŒ®

1. **OpenAI Embeddings**: https://platform.openai.com/docs/guides/embeddings
2. **æ™ºè°±AI**: https://open.bigmodel.cn/dev/api#text_embedding
3. **é˜¿é‡Œé€šä¹‰**: https://help.aliyun.com/document_detail/2587498.html
4. **Multi-Modal Fusion**: "Multimodal Learning with Transformers" (ACL 2022)
5. **News Recommendation**: "Neural News Recommendation with Multi-Head Self-Attention" (EMNLP 2019)

---

**æ–¹æ¡ˆåˆ¶å®šæ—¥æœŸ**: 2025-11-29
**ç‰ˆæœ¬**: v1.0
**çŠ¶æ€**: ğŸ“‹ å¾…å®æ–½
