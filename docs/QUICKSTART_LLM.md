# LLM Embedding å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“‹ å‡†å¤‡å·¥ä½œ

### 1. å®‰è£…ä¾èµ–

```bash
pip install openai tenacity
```

### 2. è·å– OpenAI API Key

1. è®¿é—® https://platform.openai.com/api-keys
2. åˆ›å»ºæ–°çš„ API key
3. å¤åˆ¶å¹¶ä¿å­˜ (æ ¼å¼: `sk-...`)

---

## ğŸš€ å¿«é€ŸéªŒè¯ (Tiny æ•°æ®é›†)

### Step 1: é¢„è®¡ç®— LLM Embeddings

åœ¨ tiny æ•°æ®é›† (500 æ¡æ–°é—») ä¸Šé¢„è®¡ç®— embeddings:

```bash
python src/precompute_llm_embeddings.py \
    --api_key sk-YOUR_API_KEY_HERE \
    --news_path data/mind_tiny/news.tsv \
    --output_path data/mind_tiny/llm_embeddings.npy \
    --model text-embedding-3-small \
    --batch_size 100
```

**é¢„æœŸ:**
- è¿è¡Œæ—¶é—´: ~2-3 åˆ†é’Ÿ
- API è°ƒç”¨: 5 æ¬¡ (500 / 100)
- æˆæœ¬: **$0.001** (çº¦ Â¥0.007, ä¸åˆ° 1 åˆ†é’±)
- è¾“å‡ºæ–‡ä»¶:
  - `llm_embeddings.npy` (500 Ã— 1536 ç»´)
  - `llm_embeddings_mapping.pkl` (ID æ˜ å°„)
  - `llm_embeddings_metadata.txt` (å…ƒæ•°æ®)

### Step 2: è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹

#### å®éªŒ 1: ä»… LLM (å¯¹æ¯”åŸºçº¿)

```bash
python src/train_llm.py \
    --epochs 3 \
    --batch_size 64 \
    --use_llm \
    --no_gnn \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy \
    --fusion_method attention
```

#### å®éªŒ 2: LLM + GNN (å®Œæ•´æ¨¡å‹)

```bash
python src/train_llm.py \
    --epochs 3 \
    --batch_size 64 \
    --use_llm \
    --use_gnn \
    --gnn_layers 2 \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy \
    --fusion_method attention
```

**é¢„æœŸè®­ç»ƒæ—¶é—´**: ~5 åˆ†é’Ÿ (CPU) / ~2 åˆ†é’Ÿ (GPU)

### Step 3: é¢„æµ‹å’Œè¯„ä¼°

```bash
python src/predict_llm.py \
    --model_path output/llm_gnn/best_model.pth \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy
```

### Step 4: æŸ¥çœ‹ç»“æœ

```bash
# æŸ¥çœ‹è¯„ä¼°æŒ‡æ ‡
cat output/llm_gnn/eval/metrics.json

# æŸ¥çœ‹ TensorBoard
tensorboard --logdir output/
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœæå‡

### Tiny æ•°æ®é›† (500 news, 3 epochs)

| æ¨¡å‹ | AUC | MRR | nDCG@10 |
|------|-----|-----|---------|
| Baseline (ID only) | 0.50 | 0.27 | 0.18 |
| +GNN | 0.50 | 0.27 | 0.18 |
| **+LLM** | **0.55** | **0.32** | **0.23** |
| **+LLM+GNN** | **0.58** | **0.35** | **0.25** |

**é¢„æœŸæå‡**:
- AUC: +0.08 (+16%)
- MRR: +0.08 (+30%)
- nDCG@10: +0.07 (+40%)

---

## ğŸ”§ å‚æ•°è°ƒä¼˜å»ºè®®

### èåˆæ–¹æ³•å¯¹æ¯”

```bash
# æ³¨æ„åŠ›èåˆ (æ¨è)
--fusion_method attention

# é—¨æ§èåˆ
--fusion_method gate

# æ‹¼æ¥èåˆ (æœ€ç®€å•)
--fusion_method concat
```

### è¾“å‡ºç»´åº¦è°ƒæ•´

```bash
# é»˜è®¤: 256 ç»´
--output_dim 256

# æ›´å¤§å®¹é‡ (å¦‚æœæ•°æ®è¶³å¤Ÿ)
--output_dim 512

# æ›´å°å®¹é‡ (é˜²æ­¢è¿‡æ‹Ÿåˆ)
--output_dim 128
```

---

## ğŸ’° æˆæœ¬ä¼°ç®—

### Tiny æ•°æ®é›† (500 news)

| æ¨¡å‹ | Tokens | API è°ƒç”¨ | æˆæœ¬ (USD) | æˆæœ¬ (CNY) |
|------|--------|---------|-----------|-----------|
| text-embedding-3-small | 50K | 5 | $0.001 | Â¥0.007 |
| text-embedding-3-large | 50K | 5 | $0.0065 | Â¥0.045 |

### å®Œæ•´ MIND-small (51K news)

| æ¨¡å‹ | Tokens | API è°ƒç”¨ | æˆæœ¬ (USD) | æˆæœ¬ (CNY) |
|------|--------|---------|-----------|-----------|
| text-embedding-3-small | 5M | 510 | $0.10 | Â¥0.70 |
| text-embedding-3-large | 5M | 510 | $0.65 | Â¥4.50 |

**ç»“è®º**: æˆæœ¬æä½ï¼Œå®Œå…¨å¯ä»¥å¿«é€Ÿè¿­ä»£å®éªŒ

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: API è°ƒç”¨å¤±è´¥

```python
# é”™è¯¯ä¿¡æ¯
RateLimitError: Rate limit reached

# è§£å†³æ–¹æ¡ˆ
--batch_size 50  # å‡å°æ‰¹æ¬¡
# æˆ–ç­‰å¾…å‡ åˆ†é’Ÿåé‡è¯•
```

### Q2: å†…å­˜ä¸è¶³

```python
# é”™è¯¯ä¿¡æ¯
RuntimeError: CUDA out of memory

# è§£å†³æ–¹æ¡ˆ 1: ä½¿ç”¨ CPU
# (ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œæ— éœ€è®¾ç½®)

# è§£å†³æ–¹æ¡ˆ 2: å‡å°æ‰¹æ¬¡
--batch_size 32
```

### Q3: LLM embeddings æœªåŠ è½½

```bash
# ç¡®è®¤æ–‡ä»¶å­˜åœ¨
ls data/mind_tiny/llm_embeddings.npy

# ç¡®è®¤è·¯å¾„æ­£ç¡®
--llm_embedding_path data/mind_tiny/llm_embeddings.npy
```

### Q4: æ•ˆæœæå‡ä¸æ˜æ˜¾

**å¯èƒ½åŸå› **:
1. æ•°æ®é›†å¤ªå° (è¯•è¯•å®Œæ•´æ•°æ®é›†)
2. è®­ç»ƒä¸å……åˆ† (å¢åŠ  epochs)
3. èåˆæ–¹æ³•ä¸å½“ (è¯•è¯• `attention`)
4. è¿‡æ‹Ÿåˆ (æ£€æŸ¥ train vs val æ›²çº¿)

---

## ğŸ”¬ å®éªŒå¯¹æ¯”è„šæœ¬

åˆ›å»ºä¸€ä¸ªå¯¹æ¯”è„šæœ¬,è‡ªåŠ¨è¿è¡Œæ‰€æœ‰å®éªŒ:

```bash
# compare_all.sh

#!/bin/bash

echo "===== Experiment 1: Baseline (ID only) ====="
python src/train_llm.py \
    --epochs 3 \
    --no_llm \
    --no_gnn

echo "===== Experiment 2: +GNN ====="
python src/train_llm.py \
    --epochs 3 \
    --no_llm \
    --use_gnn

echo "===== Experiment 3: +LLM ====="
python src/train_llm.py \
    --epochs 3 \
    --use_llm \
    --no_gnn \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy

echo "===== Experiment 4: +LLM+GNN ====="
python src/train_llm.py \
    --epochs 3 \
    --use_llm \
    --use_gnn \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy

echo "===== Comparing Results ====="
python compare_results.py
```

---

## ğŸ“ˆ TensorBoard å®æ—¶ç›‘æ§

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir output/

# è®¿é—®
http://localhost:6006
```

**å¯è§†åŒ–å†…å®¹**:
- è®­ç»ƒ/éªŒè¯æŸå¤±
- è®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡
- å­¦ä¹ ç‡å˜åŒ–
- ä¸åŒæ¨¡å‹çš„å¯¹æ¯”

---

## ğŸ¯ ä¸‹ä¸€æ­¥

### 1. æ‰©å±•åˆ°å®Œæ•´æ•°æ®é›†

```bash
# é¢„è®¡ç®—å®Œæ•´æ•°æ®é›† (51K news)
python src/precompute_llm_embeddings.py \
    --api_key sk-... \
    --news_path data/mind_small/train/news.tsv \
    --output_path data/mind_small/llm_embeddings.npy

# è®­ç»ƒ (ä¿®æ”¹æ•°æ®åŠ è½½ä¸ºå®Œæ•´æ•°æ®é›†)
# éœ€è¦ä¿®æ”¹ train_llm.py ä¸­çš„ get_tiny_dataloaders
```

### 2. å°è¯•å…¶ä»–èåˆæ–¹æ³•

```bash
# Gate fusion
--fusion_method gate

# Concat fusion
--fusion_method concat
```

### 3. è¶…å‚æ•°è°ƒä¼˜

```bash
# æ›´å¤§çš„è¾“å‡ºç»´åº¦
--output_dim 512

# æ›´æ·±çš„ GNN
--gnn_layers 3

# æ›´ä½çš„å­¦ä¹ ç‡
--lr 0.0005
```

### 4. åˆ†ææ³¨æ„åŠ›æƒé‡

æŸ¥çœ‹æ¨¡å‹å­¦ä¹ åˆ°çš„èåˆæƒé‡:
- ID embedding æƒé‡
- LLM embedding æƒé‡
- GNN embedding æƒé‡

---

## ğŸ’¡ Tips

1. **å…ˆåœ¨ tiny ä¸ŠéªŒè¯**: å¿«é€Ÿè¿­ä»£,æˆæœ¬ä½
2. **ä½¿ç”¨ TensorBoard**: å®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹
3. **ä¿å­˜æ‰€æœ‰å®éªŒç»“æœ**: æ–¹ä¾¿å¯¹æ¯”
4. **æ³¨æ„è¿‡æ‹Ÿåˆ**: ç›‘æ§ train/val gap
5. **GPU åŠ é€Ÿ**: å¦‚æœ‰ GPU,å¯å¤§å¹…ç¼©çŸ­è®­ç»ƒæ—¶é—´

---

## ğŸ“ æ”¯æŒ

é‡åˆ°é—®é¢˜? æŸ¥çœ‹:
- è¯¦ç»†æ–‡æ¡£: `LLM_EMBEDDING_PROPOSAL.md`
- æŠ€æœ¯æ–¹æ¡ˆ: `GNN_README.md`
- OpenAI æ–‡æ¡£: https://platform.openai.com/docs/guides/embeddings

---

**å‡†å¤‡å¥½äº†å—? è®©æˆ‘ä»¬å¼€å§‹å§!** ğŸš€

```bash
# ä¸€é”®è¿è¡Œ (éœ€è¦å…ˆè®¾ç½® API key)
export OPENAI_API_KEY=sk-...

python src/precompute_llm_embeddings.py \
    --api_key $OPENAI_API_KEY \
    --news_path data/mind_tiny/news.tsv \
    --output_path data/mind_tiny/llm_embeddings.npy

python src/train_llm.py \
    --epochs 3 \
    --use_llm \
    --use_gnn \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy
```
