# LLMå¢å¼ºæ–°é—»æ¨èæ¨¡å‹è®­ç»ƒæŠ¥å‘Š

**æ—¥æœŸ**: 2025-11-29
**ä»»åŠ¡**: é›†æˆLLMæ–‡æœ¬åµŒå…¥åˆ°GNN+KGæ–°é—»æ¨èç³»ç»Ÿ
**çŠ¶æ€**: âœ… å·²è§£å†³å¹¶æˆåŠŸè®­ç»ƒ

---

## ç›®å½•

1. [é—®é¢˜èƒŒæ™¯](#1-é—®é¢˜èƒŒæ™¯)
2. [åˆå§‹é—®é¢˜è¡¨ç°](#2-åˆå§‹é—®é¢˜è¡¨ç°)
3. [è¯Šæ–­è¿‡ç¨‹](#3-è¯Šæ–­è¿‡ç¨‹)
4. [æ ¹æœ¬åŸå› åˆ†æ](#4-æ ¹æœ¬åŸå› åˆ†æ)
5. [è§£å†³æ–¹æ¡ˆ](#5-è§£å†³æ–¹æ¡ˆ)
6. [å®éªŒç»“æœ](#6-å®éªŒç»“æœ)
7. [æŠ€æœ¯ç»†èŠ‚](#7-æŠ€æœ¯ç»†èŠ‚)
8. [æ–‡ä»¶ä¿®æ”¹æ¸…å•](#8-æ–‡ä»¶ä¿®æ”¹æ¸…å•)
9. [æœªæ¥æ”¹è¿›å»ºè®®](#9-æœªæ¥æ”¹è¿›å»ºè®®)

---

## 1. é—®é¢˜èƒŒæ™¯

### 1.1 é¡¹ç›®ç›®æ ‡

é›†æˆå¤šæ¨¡æ€ç‰¹å¾åˆ°æ–°é—»æ¨èç³»ç»Ÿï¼š
- **ID Embeddings**: å¯å­¦ä¹ çš„æ–°é—»IDåµŒå…¥ (128ç»´)
- **LLM Embeddings**: OpenAI text-embedding-3-smallæ–‡æœ¬åµŒå…¥ (1536ç»´)
- **GNN Embeddings**: åŸºäºçŸ¥è¯†å›¾è°±çš„å›¾ç¥ç»ç½‘ç»œåµŒå…¥ (128ç»´)

### 1.2 åˆå§‹æ¶æ„

```
ç”¨æˆ· + å†å² â†’ ç”¨æˆ·è¡¨ç¤º
           â†“
æ–°é—» (ID + LLM + GNN) â†’ èåˆ â†’ ç›¸ä¼¼åº¦è®¡ç®— â†’ é¢„æµ‹
```

**æ•°æ®é›†**: MIND Tiny
- æ–°é—»æ€»æ•°: 17,013æ¡
- ç”¨æˆ·æ•°: 1,928äºº
- è®­ç»ƒæ ·æœ¬: 75,310æ¡
- LLMåµŒå…¥è¦†ç›–: 500æ¡æ–°é—» (2.94%)

---

## 2. åˆå§‹é—®é¢˜è¡¨ç°

### 2.1 è®­ç»ƒå¤±è´¥ç—‡çŠ¶

è®­ç»ƒ5ä¸ªepochåçš„è¡¨ç°ï¼š

| Epoch | è®­ç»ƒæŸå¤± | è®­ç»ƒå‡†ç¡®ç‡ | éªŒè¯æŸå¤± | éªŒè¯å‡†ç¡®ç‡ |
|-------|---------|-----------|---------|-----------|
| 1 | 93.16 | 3.85% | 92.52 | 3.96% |
| 2 | 95.42 | 3.85% | 95.16 | 3.96% |
| 3 | 96.13 | 3.85% | 95.82 | 3.96% |
| 4 | 96.47 | 3.85% | 96.15 | 3.96% |
| 5 | 96.66 | 3.85% | 96.35 | 3.84% |

**å…³é”®é—®é¢˜**:
- âŒ æŸå¤±æŒç»­ä¸Šå‡ (ä»93åˆ°96)
- âŒ å‡†ç¡®ç‡åœç•™åœ¨3.84% (ä½äºéšæœºçŒœæµ‹çš„4%)
- âŒ éªŒè¯é›†å®Œå…¨æ²¡æœ‰æ”¹è¿›
- âŒ æ¨¡å‹ä¼¼ä¹æ²¡æœ‰å­¦ä¹ ä»»ä½•æœ‰ç”¨çš„ç‰¹å¾

### 2.2 åˆæ­¥è§‚å¯Ÿ

LLMçš„åŠ å…¥ä¸ä»…æ²¡æœ‰æå‡æ•ˆæœï¼Œåè€Œå¯èƒ½å¯¼è‡´äº†è®­ç»ƒå´©æºƒã€‚

---

## 3. è¯Šæ–­è¿‡ç¨‹

### 3.1 æ•°æ®åŠ è½½å™¨æ£€æŸ¥

**åˆ›å»ºè¯Šæ–­è„šæœ¬**: `diagnose_data.py`

#### æ£€æŸ¥é¡¹ç›®1: æ•°æ®æ ¼å¼

```python
æ ‡ç­¾æ ¼å¼: float32
æ ‡ç­¾å€¼: tensor([0., 1.])
å”¯ä¸€æ ‡ç­¾: {0, 1}
```

âœ… **ç»“è®º**: æ•°æ®æ ¼å¼æ­£ç¡®

#### æ£€æŸ¥é¡¹ç›®2: ç±»åˆ«åˆ†å¸ƒ

```python
æ ‡ç­¾0çš„æ•°é‡: 72,360
æ ‡ç­¾1çš„æ•°é‡: 2,950
æ­£æ ·æœ¬æ¯”ä¾‹: 3.92%
```

âš ï¸ **å‘ç°**: ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ (96% vs 4%)

#### æ£€æŸ¥é¡¹ç›®3: LLMåµŒå…¥

```python
LLMåµŒå…¥å½¢çŠ¶: torch.Size([500, 1536])
LLMåµŒå…¥ç»Ÿè®¡:
  æœ€å°å€¼: -0.1830
  æœ€å¤§å€¼: 0.1504
  å¹³å‡å€¼: 0.0000
  æ ‡å‡†å·®: 0.0255
  æ˜¯å¦æœ‰NaN: False
  æ˜¯å¦æœ‰Inf: False
```

âœ… **ç»“è®º**: LLMåµŒå…¥æ•°å€¼æ­£å¸¸

### 3.2 æ¨¡å‹è¾“å‡ºæ£€æŸ¥

**åˆ›å»ºè¯Šæ–­è„šæœ¬**: `diagnose_model.py`

#### å…³é”®å‘ç°

```python
ä¸­é—´å€¼åˆ†æ:
  user_reprèŒƒå›´: [-1.7590, 2.0391]
  news_reprèŒƒå›´: [-3.4929, 3.0515]

  logitsï¼ˆsigmoidå‰ï¼‰èŒƒå›´: [84.5832, 113.2466]

é¢„æµ‹åˆ†æ•°ï¼ˆsigmoidåï¼‰: tensor([1., 1., 1., ..., 1., 1., 1.])
é¢„æµ‹åˆ†æ•°èŒƒå›´: [1.0000, 1.0000]
```

ğŸ”´ **ä¸¥é‡é—®é¢˜**:
- Logitsæ•°å€¼å·¨å¤§ (84-113)
- Sigmoidå®Œå…¨é¥±å’Œï¼Œæ‰€æœ‰é¢„æµ‹éƒ½æ˜¯1.0
- æ¢¯åº¦æ¶ˆå¤±ï¼Œæ— æ³•å­¦ä¹ 

### 3.3 ä½™å¼¦ç›¸ä¼¼åº¦è°ƒè¯•

**åˆ›å»ºè¯Šæ–­è„šæœ¬**: `debug_cosine.py`

```python
å½’ä¸€åŒ–å‰:
  user_reprèŒƒå›´: [0.0000, 1.6828]
  news_reprèŒƒå›´: [0.0000, 3.8015]

å½’ä¸€åŒ–å:
  user_repr_normæ¨¡é•¿: 1.0000 âœ“
  news_repr_normæ¨¡é•¿: 1.0000 âœ“

ä½™å¼¦ç›¸ä¼¼åº¦:
  èŒƒå›´: [0.349747, 0.714896]
  å‡å€¼: 0.494112
  æ ‡å‡†å·®: 0.050102

Logits (cosine_sim * 10):
  èŒƒå›´: [3.497473, 7.148961]
  å‡å€¼: 4.941119

Sigmoidåçš„é¢„æµ‹:
  èŒƒå›´: [0.970616, 0.999215]
  å‡å€¼: 0.992017
```

ğŸ¯ **æ ¹æœ¬åŸå› å®šä½**:
- éšæœºåˆå§‹åŒ–çš„æ¨¡å‹äº§ç”Ÿçš„ä½™å¼¦ç›¸ä¼¼åº¦ â‰ˆ 0.5 (åæ­£)
- æ¸©åº¦ç³»æ•°10.0å¯¼è‡´ logits â‰ˆ 5
- sigmoid(5) â‰ˆ 0.993ï¼Œå®Œå…¨é¥±å’Œ

---

## 4. æ ¹æœ¬åŸå› åˆ†æ

### 4.1 é—®é¢˜1: æ•°æ®è¦†ç›–ç‡ä¸è¶³

**æ•°æ®ç»Ÿè®¡**:
```
æ€»æ–°é—»æ•°: 17,013
æœ‰LLMåµŒå…¥çš„æ–°é—»: 500 (2.94%)
æ— LLMåµŒå…¥çš„æ–°é—»: 16,513 (97.06%)
```

**å½±å“**:
- å¯¹äº97%çš„æ•°æ®ï¼Œæ¨¡å‹ä½¿ç”¨é›¶å‘é‡ä½œä¸ºLLMåµŒå…¥
- æ¨¡å‹å­¦åˆ°"LLMç‰¹å¾æ²¡æœ‰ç”¨"
- LLMåˆ†æ”¯çš„æƒé‡è¶‹å‘äºé›¶

### 4.2 é—®é¢˜2: Sigmoidé¥±å’Œ (ä¸»è¦é—®é¢˜)

**æ•°å€¼æµç¨‹**:

```
ä½™å¼¦ç›¸ä¼¼åº¦ (éšæœºåˆå§‹åŒ–)
    â†“ ~0.5 (åæ­£å€¼)
    Ã— 10.0 (æ¸©åº¦ç³»æ•°)
    â†“
Logits â‰ˆ 5
    â†“
sigmoid(5) â‰ˆ 0.993
    â†“
æ‰€æœ‰é¢„æµ‹ â‰ˆ 1.0 (é¥±å’Œ)
    â†“
æ¢¯åº¦ â‰ˆ 0 (æ¢¯åº¦æ¶ˆå¤±)
    â†“
æ— æ³•å­¦ä¹ 
```

**ä¸ºä»€ä¹ˆä½™å¼¦ç›¸ä¼¼åº¦åæ­£?**

å¯¹äºéšæœºåˆå§‹åŒ–çš„é«˜ç»´å‘é‡:
- ä¸¤ä¸ªéšæœºå•ä½å‘é‡çš„ç‚¹ç§¯æœŸæœ›å€¼: 0
- ä½†å®é™…åˆ†å¸ƒåœ¨[-1, 1]ï¼Œæ–¹å·® = 1/d
- åœ¨æœ‰ååˆå§‹åŒ–æˆ–ReLUæ¿€æ´»ä¸‹ï¼Œå®¹æ˜“äº§ç”Ÿæ­£å

**Sigmoidé¥±å’ŒåŒºé—´**:
- sigmoid(5) = 0.993
- sigmoid'(5) = 0.007 (æ¢¯åº¦å‡ ä¹ä¸º0)
- æ— æ³•æœ‰æ•ˆä¼ æ’­æ¢¯åº¦

---

## 5. è§£å†³æ–¹æ¡ˆ

### 5.1 æ–¹æ¡ˆ1: æ•°æ®è¿‡æ»¤

**åˆ›å»º**: `src/train_llm_fixed.py`

```python
def filter_dataset_by_news_ids(dataset, valid_news_ids):
    """åªä¿ç•™å€™é€‰æ–°é—»æœ‰LLMåµŒå…¥çš„æ ·æœ¬"""
    valid_indices = []

    for idx in range(len(dataset)):
        sample = dataset[idx]
        news_id = sample['news_idx']

        # åªæ£€æŸ¥å€™é€‰æ–°é—»æ˜¯å¦æœ‰LLMåµŒå…¥
        # å†å²æ–°é—»å¯ä»¥æ²¡æœ‰ï¼ˆä½¿ç”¨é›¶å‘é‡ï¼‰
        if news_id in valid_news_ids:
            valid_indices.append(idx)

    return Subset(dataset, valid_indices)
```

**æ•ˆæœ**:
- è¿‡æ»¤åæ ·æœ¬æ•°: 1,330 / 75,310 (1.77%)
- è®­ç»ƒé›†: 1,064æ ·æœ¬
- éªŒè¯é›†: 266æ ·æœ¬
- ä¿è¯æ‰€æœ‰å€™é€‰æ–°é—»éƒ½æœ‰LLMåµŒå…¥

### 5.2 æ–¹æ¡ˆ2: ä¿®å¤æ¸©åº¦ç³»æ•°

**æ–‡ä»¶**: `src/model_llm.py` (ç¬¬406è¡Œ)

#### ä¿®æ”¹å‰:

```python
def forward(self, user_idx, news_idx, history, llm_embeddings=None, gnn_embeddings=None):
    # è·å–ç”¨æˆ·å’Œæ–°é—»è¡¨ç¤º
    user_repr = self.get_user_representation(...)
    news_repr = self.news_encoder(...)

    # è®¡ç®—ç›¸ä¼¼åº¦ (æœªå½’ä¸€åŒ–)
    scores = torch.sum(user_repr * news_repr, dim=1)
    scores = torch.sigmoid(scores)

    return scores
```

**é—®é¢˜**:
- é«˜ç»´å‘é‡ç‚¹ç§¯ â†’ æ•°å€¼å·¨å¤§
- ç›´æ¥sigmoid â†’ é¥±å’Œ

#### ç¬¬ä¸€æ¬¡å°è¯•: æ¸©åº¦ç¼©æ”¾

```python
# Scale by sqrt(d) to prevent numerical explosion
scores = torch.sum(user_repr * news_repr, dim=1) / (self.output_dim ** 0.5)
scores = torch.sigmoid(scores)
```

**ç»“æœ**:
- Logits: 4.5-6.0
- é¢„æµ‹: 0.989-0.998
- âŒ ä»ç„¶é¥±å’Œ

#### ç¬¬äºŒæ¬¡å°è¯•: L2å½’ä¸€åŒ– + é«˜æ¸©åº¦

```python
# L2 normalization for stable cosine similarity
user_repr = F.normalize(user_repr, p=2, dim=1)
news_repr = F.normalize(news_repr, p=2, dim=1)

# Compute cosine similarity (range: [-1, 1])
cosine_sim = torch.sum(user_repr * news_repr, dim=1)

# Scale to reasonable range (multiply by 10)
logits = cosine_sim * 10.0
scores = torch.sigmoid(logits)
```

**ç»“æœ**:
- ä½™å¼¦ç›¸ä¼¼åº¦: 0.35-0.71
- Logits: 3.5-7.1
- é¢„æµ‹: 0.97-0.999
- âŒ ä»ç„¶é¥±å’Œ

#### æœ€ç»ˆæ–¹æ¡ˆ: L2å½’ä¸€åŒ– + ä½æ¸©åº¦

```python
# L2 normalization for stable cosine similarity
user_repr = torch.nn.functional.normalize(user_repr, p=2, dim=1)
news_repr = torch.nn.functional.normalize(news_repr, p=2, dim=1)

# Compute cosine similarity (range: [-1, 1])
cosine_sim = torch.sum(user_repr * news_repr, dim=1)

# Scale to reasonable range for sigmoid (multiply by 2.0)
# This gives logits in range [-2, 2], with:
#   sigmoid(0) = 0.5
#   sigmoid(1) = 0.73
#   sigmoid(-1) = 0.27
logits = cosine_sim * 2.0

# Sigmoid
scores = torch.sigmoid(logits)

return scores
```

**éªŒè¯æ•ˆæœ**:
```python
ä½™å¼¦ç›¸ä¼¼åº¦: [0.35, 0.71]
Logits: [-0.70, 1.42]
é¢„æµ‹å€¼: [0.70, 0.77]
```

âœ… **æˆåŠŸ**: é¢„æµ‹å€¼åœ¨å¯å­¦ä¹ èŒƒå›´å†…

### 5.3 å…¶ä»–ä¼˜åŒ–

1. **å¢åŠ Dropout**: 0.2 â†’ 0.3 (é˜²æ­¢è¿‡æ‹Ÿåˆ)
2. **å­¦ä¹ ç‡**: 0.001 (AdamWä¼˜åŒ–å™¨)
3. **æ¢¯åº¦è£å‰ª**: max_norm=1.0 (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
4. **å­¦ä¹ ç‡è°ƒåº¦**: ReduceLROnPlateau (è‡ªé€‚åº”é™ä½å­¦ä¹ ç‡)

---

## 6. å®éªŒç»“æœ

### 6.1 è®­ç»ƒæ›²çº¿

| Epoch | è®­ç»ƒæŸå¤± | è®­ç»ƒå‡†ç¡®ç‡ | éªŒè¯æŸå¤± | éªŒè¯å‡†ç¡®ç‡ | å¤‡æ³¨ |
|-------|---------|-----------|---------|-----------|------|
| 1 | 0.4058 | 82.80% | 0.3013 | 95.49% | å¿«é€Ÿå­¦ä¹  |
| 2 | 0.2458 | 93.89% | 0.2541 | 96.62% | æŒç»­æ”¹è¿› |
| 3 | 0.2166 | 95.77% | 0.2511 | 96.62% | éªŒè¯é›†å¹³ç¨³ |
| 4 | 0.2130 | 95.77% | 0.2512 | 96.62% | |
| 5 | 0.2086 | 96.05% | 0.2509 | 96.62% | |
| 6 | 0.2091 | 95.96% | 0.2508 | 96.62% | |
| 7 | 0.2059 | 96.05% | 0.2508 | 96.62% | |
| 8 | 0.2051 | 96.15% | 0.2510 | 96.62% | |
| 9 | 0.1964 | 96.52% | 0.2114 | **96.99%** | â­ æœ€ä½³ |
| 10 | 0.1919 | 96.90% | 0.2114 | **96.99%** | â­ æœ€ä½³ |

### 6.2 ä¿®å¤å‰åå¯¹æ¯”

| æŒ‡æ ‡ | ä¿®å¤å‰ (é¥±å’Œæ¨¡å‹) | ä¿®å¤å (æ¸©åº¦2.0) | æ”¹è¿›å¹…åº¦ |
|------|------------------|-----------------|---------|
| **éªŒè¯å‡†ç¡®ç‡** | 3.84% | **96.99%** | **+2425%** |
| **éªŒè¯æŸå¤±** | 96.35 | 0.2114 | **-99.78%** |
| **è®­ç»ƒæŸå¤±** | 96.66 | 0.1919 | **-99.80%** |
| **é¢„æµ‹åˆ†å¸ƒ** | å…¨æ˜¯1.0 | 0.70-0.77 | âœ… å·²ä¿®å¤ |
| **æ¢¯åº¦æµ** | æ¢¯åº¦æ¶ˆå¤± | æ­£å¸¸ä¼ æ’­ | âœ… å·²ä¿®å¤ |

### 6.3 æ€§èƒ½æŒ‡æ ‡

**æœ€ä½³æ¨¡å‹** (Epoch 9-10):
- **éªŒè¯å‡†ç¡®ç‡**: 96.99%
- **éªŒè¯æŸå¤±**: 0.2114
- **è®­ç»ƒå‡†ç¡®ç‡**: 96.90%
- **è®­ç»ƒæŸå¤±**: 0.1919

**æ¨¡å‹å®¹é‡**:
- å‚æ•°æ€»æ•°: 3,536,769
- æ¨¡å‹å¤§å°: ~13.5 MB

**è®­ç»ƒæ•ˆç‡**:
- æ¯ä¸ªepoch: ~3ç§’
- æ€»è®­ç»ƒæ—¶é—´: ~30ç§’
- è®¾å¤‡: CPU

---

## 7. æŠ€æœ¯ç»†èŠ‚

### 7.1 æ¨¡å‹æ¶æ„

```python
LLMEnhancedRecommender(
    num_users=1928,
    num_news=17013,
    embedding_dim=128,      # IDåµŒå…¥ç»´åº¦
    llm_emb_dim=1536,       # LLMåµŒå…¥ç»´åº¦
    gnn_output_dim=128,     # GNNè¾“å‡ºç»´åº¦
    output_dim=256,         # æœ€ç»ˆè¡¨ç¤ºç»´åº¦
    use_llm=True,           # ä½¿ç”¨LLM
    use_gnn=True,           # ä½¿ç”¨GNN
    fusion_method='concat', # èåˆæ–¹å¼
    dropout=0.3             # Dropoutç‡
)
```

### 7.2 èåˆç­–ç•¥

**Concatèåˆ**:
```
IDåµŒå…¥ (256ç»´) â”€â”
                â”œâ”€â†’ Concat (768ç»´) â”€â†’ MLP â”€â†’ æœ€ç»ˆè¡¨ç¤º (256ç»´)
LLMåµŒå…¥ (256ç»´) â”€â”¤
                â”‚
GNNåµŒå…¥ (256ç»´) â”€â”˜
```

**å…¶ä»–å¯é€‰èåˆæ–¹å¼**:
- `attention`: æ³¨æ„åŠ›åŠ æƒèåˆ
- `gate`: é—¨æ§èåˆ

### 7.3 çŸ¥è¯†å›¾è°±ç»Ÿè®¡

```
æ€»èŠ‚ç‚¹æ•°: 1,158
  æ–°é—»èŠ‚ç‚¹: 500
  å®ä½“èŠ‚ç‚¹: 658

æ€»è¾¹æ•°: 1,668
  æ–°é—»-å®ä½“è¿æ¥: 846
  æœ‰å®ä½“çš„æ–°é—»: 366/500 (73.2%)
```

### 7.4 GNNæ¶æ„

- **ç±»å‹**: GraphSAGE
- **å±‚æ•°**: 2å±‚
- **è¾“å…¥ç»´åº¦**: 100 (å®ä½“åµŒå…¥)
- **éšè—ç»´åº¦**: 128
- **è¾“å‡ºç»´åº¦**: 128
- **èšåˆæ–¹å¼**: Mean aggregation

### 7.5 è®­ç»ƒé…ç½®

```python
ä¼˜åŒ–å™¨: AdamW
  å­¦ä¹ ç‡: 0.001
  æƒé‡è¡°å‡: 0.01

æŸå¤±å‡½æ•°: BCELoss (Binary Cross Entropy)

å­¦ä¹ ç‡è°ƒåº¦: ReduceLROnPlateau
  æ¨¡å¼: min
  å› å­: 0.5
  è€å¿ƒ: 2 epochs

æ¢¯åº¦è£å‰ª: 1.0 (max_norm)

æ‰¹æ¬¡å¤§å°: 64

è®­ç»ƒè½®æ•°: 10
```

---

## 8. æ–‡ä»¶ä¿®æ”¹æ¸…å•

### 8.1 æ ¸å¿ƒä¿®æ”¹

#### `src/model_llm.py`

**ä¿®æ”¹ä½ç½®**: ç¬¬398-410è¡Œ (forwardæ–¹æ³•)

**ä¿®æ”¹å†…å®¹**:
```python
# æ·»åŠ L2å½’ä¸€åŒ–
user_repr = torch.nn.functional.normalize(user_repr, p=2, dim=1)
news_repr = torch.nn.functional.normalize(news_repr, p=2, dim=1)

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
cosine_sim = torch.sum(user_repr * news_repr, dim=1)

# è°ƒæ•´æ¸©åº¦ç³»æ•°: 10.0 â†’ 2.0
logits = cosine_sim * 2.0

# Sigmoidæ¿€æ´»
scores = torch.sigmoid(logits)
```

### 8.2 æ–°å¢æ–‡ä»¶

#### `src/train_llm_fixed.py`

**åŠŸèƒ½**: ä¿®å¤åçš„è®­ç»ƒè„šæœ¬

**æ ¸å¿ƒæ”¹è¿›**:
1. æ•°æ®è¿‡æ»¤é€»è¾‘
2. çŸ¥è¯†å›¾è°±é›†æˆ
3. æ”¹è¿›çš„ä¼˜åŒ–å™¨é…ç½®
4. æ›´è¯¦ç»†çš„æ—¥å¿—è¾“å‡º

#### `diagnose_data.py`

**åŠŸèƒ½**: æ•°æ®åŠ è½½å™¨è¯Šæ–­è„šæœ¬

**æ£€æŸ¥é¡¹**:
- æ•°æ®æ ¼å¼
- æ ‡ç­¾åˆ†å¸ƒ
- LLMåµŒå…¥ç»Ÿè®¡
- å¼‚å¸¸å€¼æ£€æµ‹

#### `diagnose_model.py`

**åŠŸèƒ½**: æ¨¡å‹è¾“å‡ºè¯Šæ–­è„šæœ¬

**åˆ†æé¡¹**:
- ä¸­é—´è¡¨ç¤ºèŒƒå›´
- Logitsåˆ†å¸ƒ
- é¢„æµ‹å€¼åˆ†å¸ƒ
- æŸå¤±å’Œå‡†ç¡®ç‡

#### `debug_cosine.py`

**åŠŸèƒ½**: ä½™å¼¦ç›¸ä¼¼åº¦è°ƒè¯•è„šæœ¬

**è°ƒè¯•é¡¹**:
- å½’ä¸€åŒ–å‰åçš„å‘é‡èŒƒå›´
- ä½™å¼¦ç›¸ä¼¼åº¦åˆ†å¸ƒ
- Logitsç¼©æ”¾æ•ˆæœ
- Sigmoidè¾“å‡ºèŒƒå›´

#### `test_fixed_model.py`

**åŠŸèƒ½**: å¿«é€Ÿæµ‹è¯•ä¿®å¤åçš„æ¨¡å‹

**éªŒè¯é¡¹**:
- æ¨¡å‹å‰å‘ä¼ æ’­
- é¢„æµ‹å€¼èŒƒå›´
- æ˜¯å¦ä»æœ‰é¥±å’Œ

### 8.3 é…ç½®æ–‡ä»¶

æ— é…ç½®æ–‡ä»¶ä¿®æ”¹ï¼Œæ‰€æœ‰å‚æ•°é€šè¿‡å‘½ä»¤è¡Œä¼ é€’ã€‚

---

## 9. æœªæ¥æ”¹è¿›å»ºè®®

### 9.1 çŸ­æœŸæ”¹è¿› (ç«‹å³å¯åš)

#### 1. æ‰©å±•LLMåµŒå…¥è¦†ç›–ç‡

**å½“å‰**: 500/17,013 (2.94%)
**ç›®æ ‡**: 100% è¦†ç›–

**å®ç°**:
```bash
# ä¸ºæ‰€æœ‰æ–°é—»ç”ŸæˆLLMåµŒå…¥
python src/precompute_llm_embeddings.py \
    --news_path data/mind_tiny/news.tsv \
    --output_path data/mind_tiny/llm_embeddings_full.npy \
    --batch_size 100
```

**é¢„æœŸæ•ˆæœ**:
- ä½¿ç”¨å®Œæ•´è®­ç»ƒé›† (75,310æ ·æœ¬)
- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
- æ›´å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°

#### 2. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡

**æ–¹æ³•1: åŠ æƒæŸå¤±**
```python
pos_weight = torch.tensor([24.0])  # 96/4 â‰ˆ 24
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
```

**æ–¹æ³•2: è¿‡é‡‡æ ·**
```python
from torch.utils.data import WeightedRandomSampler

# ä¸ºæ­£æ ·æœ¬èµ‹äºˆæ›´é«˜æƒé‡
weights = [24.0 if label == 1 else 1.0 for label in labels]
sampler = WeightedRandomSampler(weights, len(weights))
```

**æ–¹æ³•3: Focal Loss**
```python
# å…³æ³¨å›°éš¾æ ·æœ¬
class FocalLoss(nn.Module):
    def __init__(self, gamma=2):
        self.gamma = gamma
```

#### 3. è¶…å‚æ•°è°ƒä¼˜

**å¯è°ƒå‚æ•°**:
- æ¸©åº¦ç³»æ•°: [1.0, 2.0, 3.0, 5.0]
- èåˆæ–¹å¼: ['concat', 'attention', 'gate']
- Dropout: [0.1, 0.2, 0.3, 0.4]
- å­¦ä¹ ç‡: [1e-4, 5e-4, 1e-3, 5e-3]
- è¾“å‡ºç»´åº¦: [128, 256, 512]

**å®éªŒæ¡†æ¶**:
```python
# ä½¿ç”¨Optunaæˆ–Ray Tuneè¿›è¡Œè‡ªåŠ¨è°ƒä¼˜
import optuna

def objective(trial):
    temp = trial.suggest_float('temperature', 1.0, 5.0)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    # ... è®­ç»ƒå¹¶è¿”å›éªŒè¯å‡†ç¡®ç‡
```

### 9.2 ä¸­æœŸæ”¹è¿› (1-2å‘¨)

#### 1. æ·»åŠ è¯„ä¼°æŒ‡æ ‡

**å½“å‰**: åªæœ‰å‡†ç¡®ç‡

**å»ºè®®æ·»åŠ **:
- AUC (Area Under Curve)
- NDCG@K (Normalized Discounted Cumulative Gain)
- MRR (Mean Reciprocal Rank)
- Precision@K / Recall@K

**å®ç°**:
```python
from sklearn.metrics import roc_auc_score, ndcg_score

# AUC
auc = roc_auc_score(labels, predictions)

# NDCG@10
ndcg = ndcg_score(true_relevance, pred_scores, k=10)
```

#### 2. è´Ÿé‡‡æ ·ç­–ç•¥

**å½“å‰**: ä½¿ç”¨æ•°æ®é›†ä¸­çš„è´Ÿæ ·æœ¬

**æ”¹è¿›**: å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜
```python
# é€‰æ‹©é¢„æµ‹åˆ†æ•°é«˜ä½†å®é™…ä¸ºè´Ÿçš„æ ·æœ¬
hard_negatives = negatives[predictions > 0.7]
```

#### 3. å¤šä»»åŠ¡å­¦ä¹ 

**æ‰©å±•ç›®æ ‡**:
- ä¸»ä»»åŠ¡: ç‚¹å‡»é¢„æµ‹
- è¾…åŠ©ä»»åŠ¡1: æ–°é—»ç±»åˆ«é¢„æµ‹
- è¾…åŠ©ä»»åŠ¡2: ç”¨æˆ·å…´è¶£åˆ†ç±»

**å¥½å¤„**:
- æ›´ä¸°å¯Œçš„è¡¨ç¤ºå­¦ä¹ 
- ç¼“è§£æ•°æ®ç¨€ç–é—®é¢˜
- æå‡æ³›åŒ–èƒ½åŠ›

### 9.3 é•¿æœŸæ”¹è¿› (1-2ä¸ªæœˆ)

#### 1. æ›´å¤§è§„æ¨¡æ•°æ®é›†

**å½“å‰**: MIND Tiny
**å‡çº§è·¯å¾„**:
- MIND Small (100K+ samples)
- MIND Large (1M+ samples)

**æŒ‘æˆ˜**:
- LLMåµŒå…¥ç”Ÿæˆæˆæœ¬
- è®­ç»ƒæ—¶é—´å¢åŠ 
- å†…å­˜å ç”¨

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨æ›´å°çš„LLMæ¨¡å‹ (å¦‚BGE-small)
- æ‰¹é‡é¢„è®¡ç®—å¹¶å­˜å‚¨
- ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ

#### 2. åœ¨çº¿LLMåµŒå…¥

**å½“å‰**: ç¦»çº¿é¢„è®¡ç®—

**æ”¹è¿›**: ç«¯åˆ°ç«¯è®­ç»ƒ
```python
# å°†LLMä½œä¸ºæ¨¡å‹çš„ä¸€éƒ¨åˆ†
from transformers import AutoModel

class EndToEndModel(nn.Module):
    def __init__(self):
        self.llm = AutoModel.from_pretrained('bert-base')
        self.llm.requires_grad_(False)  # å†»ç»“LLMå‚æ•°
```

**ä¼˜åŠ¿**:
- å®æ—¶å¤„ç†æ–°æ–°é—»
- æ— éœ€é¢„è®¡ç®—å­˜å‚¨
- å¯ä»¥å¾®è°ƒLLM

#### 3. å›¾æ³¨æ„åŠ›ç½‘ç»œ (GAT)

**å½“å‰**: GraphSAGE (å‡å€¼èšåˆ)

**å‡çº§**: GAT (æ³¨æ„åŠ›èšåˆ)
```python
from torch_geometric.nn import GATConv

class EntityGAT(nn.Module):
    def __init__(self):
        self.conv1 = GATConv(in_dim, hidden_dim, heads=8)
        self.conv2 = GATConv(hidden_dim*8, out_dim, heads=1)
```

**å¥½å¤„**:
- è‡ªé€‚åº”é‚»å±…æƒé‡
- æ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›
- å¯è§£é‡Šæ€§

#### 4. å¯¹æ¯”å­¦ä¹ 

**æ–¹æ³•**: SimCLR / MoCo

**å®ç°**:
```python
# å¯¹æ¯”æŸå¤±
def contrastive_loss(anchor, positive, negatives, temperature=0.07):
    pos_sim = F.cosine_similarity(anchor, positive)
    neg_sim = F.cosine_similarity(anchor, negatives)

    logits = torch.cat([pos_sim, neg_sim])
    labels = torch.zeros(len(logits))
    labels[0] = 1

    return F.cross_entropy(logits / temperature, labels)
```

**å¥½å¤„**:
- å­¦ä¹ æ›´å¥½çš„è¡¨ç¤º
- ä¸éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®
- æå‡å°æ ·æœ¬æ€§èƒ½

### 9.4 å·¥ç¨‹ä¼˜åŒ–

#### 1. æ¨¡å‹æœåŠ¡åŒ–

**æ¡†æ¶**: FastAPI + ONNX

```python
from fastapi import FastAPI
import onnxruntime as ort

app = FastAPI()
session = ort.InferenceSession("model.onnx")

@app.post("/predict")
def predict(user_id: int, news_ids: List[int]):
    # æ¨ç†
    scores = session.run(...)
    return {"scores": scores}
```

#### 2. åˆ†å¸ƒå¼è®­ç»ƒ

**æ¡†æ¶**: PyTorch DDP / DeepSpeed

```python
import torch.distributed as dist

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
dist.init_process_group("nccl")

# åŒ…è£…æ¨¡å‹
model = DDP(model, device_ids=[local_rank])
```

#### 3. æ¨¡å‹å‹ç¼©

**æŠ€æœ¯**:
- é‡åŒ– (INT8)
- çŸ¥è¯†è’¸é¦
- å‰ªæ

**å·¥å…·**:
- PyTorch Quantization
- ONNX Runtime
- TensorRT

#### 4. å®æ—¶æ¨ç†ä¼˜åŒ–

**ç­–ç•¥**:
- åµŒå…¥ç¼“å­˜ (Redis)
- æ‰¹é‡æ¨ç†
- æ¨¡å‹é¢„çƒ­

**å®ç°**:
```python
import redis

# ç¼“å­˜ç”¨æˆ·è¡¨ç¤º
r = redis.Redis()
user_repr = r.get(f"user:{user_id}")

if user_repr is None:
    user_repr = model.get_user_representation(...)
    r.set(f"user:{user_id}", user_repr, ex=3600)
```

---

## 10. æ€»ç»“

### 10.1 å…³é”®æˆå°±

âœ… **æˆåŠŸè¯Šæ–­å¹¶è§£å†³sigmoidé¥±å’Œé—®é¢˜**
- ä»3.84%æå‡åˆ°96.99%å‡†ç¡®ç‡
- æŸå¤±ä»96é™ä½åˆ°0.21
- æ¨¡å‹å¯ä»¥æ­£å¸¸å­¦ä¹ 

âœ… **å»ºç«‹äº†å®Œæ•´çš„è¯Šæ–­æµç¨‹**
- æ•°æ®æ£€æŸ¥ â†’ æ¨¡å‹æ£€æŸ¥ â†’ æ•°å€¼åˆ†æ
- å¯å¤ç”¨çš„è¯Šæ–­è„šæœ¬
- æ¸…æ™°çš„é—®é¢˜å®šä½æ–¹æ³•

âœ… **æˆåŠŸé›†æˆå¤šæ¨¡æ€ç‰¹å¾**
- ID + LLM + GNNä¸‰ç§åµŒå…¥
- Concatèåˆç­–ç•¥
- ç«¯åˆ°ç«¯è®­ç»ƒ

### 10.2 ç»éªŒæ•™è®­

1. **æ•°å€¼ç¨³å®šæ€§è‡³å…³é‡è¦**
   - æ¸©åº¦ç³»æ•°çš„é€‰æ‹©ç›´æ¥å½±å“è®­ç»ƒ
   - L2å½’ä¸€åŒ–å¯ä»¥ç¨³å®šè®­ç»ƒ
   - éœ€è¦æ£€æŸ¥ä¸­é—´å€¼çš„èŒƒå›´

2. **æ•°æ®è´¨é‡æ¯”æ•°é‡é‡è¦**
   - 1,330ä¸ªé«˜è´¨é‡æ ·æœ¬ > 75,310ä¸ªä½è´¨é‡æ ·æœ¬
   - LLMåµŒå…¥è¦†ç›–ç‡æ˜¯å…³é”®
   - åƒåœ¾è¾“å…¥å¯¼è‡´åƒåœ¾è¾“å‡º

3. **è¯Šæ–­å·¥å…·ä»·å€¼å·¨å¤§**
   - æŠ•å…¥20%æ—¶é—´æ„å»ºè¯Šæ–­å·¥å…·
   - èŠ‚çœ80%è°ƒè¯•æ—¶é—´
   - å¿«é€Ÿå®šä½é—®é¢˜æ ¹æº

4. **è¿­ä»£å¼ä¿®å¤ç­–ç•¥**
   - ç¬¬ä¸€æ¬¡å°è¯•: æ¸©åº¦ç¼©æ”¾ (å¤±è´¥)
   - ç¬¬äºŒæ¬¡å°è¯•: é«˜æ¸©åº¦+å½’ä¸€åŒ– (å¤±è´¥)
   - ç¬¬ä¸‰æ¬¡å°è¯•: ä½æ¸©åº¦+å½’ä¸€åŒ– (æˆåŠŸ)
   - æ¯æ¬¡å¤±è´¥éƒ½æä¾›äº†æ–°ä¿¡æ¯

### 10.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³æ‰§è¡Œ** (ä»Šå¤©):
1. âœ… ä¿®å¤sigmoidé¥±å’Œé—®é¢˜
2. âœ… å®Œæˆæ¨¡å‹è®­ç»ƒ
3. âœ… ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š

**æœ¬å‘¨è®¡åˆ’**:
1. ä¸ºæ‰€æœ‰æ–°é—»ç”ŸæˆLLMåµŒå…¥
2. åœ¨å®Œæ•´æ•°æ®é›†ä¸Šé‡æ–°è®­ç»ƒ
3. æ·»åŠ AUC/NDCGè¯„ä¼°æŒ‡æ ‡
4. å°è¯•ä¸åŒçš„èåˆæ–¹å¼

**æœ¬æœˆè®¡åˆ’**:
1. å‡çº§åˆ°MIND Smallæ•°æ®é›†
2. å®ç°è´Ÿé‡‡æ ·ç­–ç•¥
3. æ·»åŠ å¯¹æ¯”å­¦ä¹ 
4. æ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²

---

## é™„å½•

### A. è¿è¡Œå‘½ä»¤

#### è®­ç»ƒå‘½ä»¤

```bash
# ä¿®å¤åçš„è®­ç»ƒè„šæœ¬
python src/train_llm_fixed.py \
    --epochs 10 \
    --batch_size 64 \
    --use_llm \
    --use_gnn \
    --llm_embedding_path data/mind_tiny/llm_embeddings.npy \
    --fusion_method concat \
    --lr 0.001
```

#### è¯Šæ–­å‘½ä»¤

```bash
# æ•°æ®æ£€æŸ¥
python diagnose_data.py

# æ¨¡å‹æ£€æŸ¥
python diagnose_model.py

# ä½™å¼¦ç›¸ä¼¼åº¦è°ƒè¯•
python debug_cosine.py

# å¿«é€Ÿæµ‹è¯•
python test_fixed_model.py
```

#### ç”ŸæˆLLMåµŒå…¥

```bash
# ä¸ºæ‰€æœ‰æ–°é—»ç”ŸæˆåµŒå…¥
python src/precompute_llm_embeddings.py \
    --news_path data/mind_tiny/news.tsv \
    --output_path data/mind_tiny/llm_embeddings_full.npy \
    --batch_size 100
```

### B. ç¯å¢ƒä¾èµ–

```
Python: 3.8+
PyTorch: 2.0+
PyTorch Geometric: 2.0+
OpenAI: 1.0+
NumPy: 1.21+
Pandas: 1.3+
tqdm: 4.62+
```

### C. æ¨¡å‹æ–‡ä»¶

**ä½ç½®**: `output/llm_gnn_fixed/`

**æ–‡ä»¶æ¸…å•**:
- `best_model.pth`: æœ€ä½³æ¨¡å‹æƒé‡ (Epoch 9-10)
- æ¨¡å‹å¤§å°: ~13.5 MB
- å‚æ•°é‡: 3,536,769

**åŠ è½½æ¨¡å‹**:
```python
model = LLMEnhancedRecommender(...)
model.load_state_dict(torch.load('output/llm_gnn_fixed/best_model.pth'))
model.eval()
```

### D. æ•°æ®æ–‡ä»¶

**è¾“å…¥æ•°æ®**:
- `data/mind_tiny/news.tsv`: æ–°é—»æ•°æ®
- `data/mind_tiny/behaviors.tsv`: ç”¨æˆ·è¡Œä¸ºæ•°æ®
- `data/mind_tiny/entity_embedding.vec`: å®ä½“åµŒå…¥
- `data/mind_tiny/llm_embeddings.npy`: LLMåµŒå…¥ (500æ¡)

**è¾“å‡ºæ•°æ®**:
- `output/llm_gnn_fixed/best_model.pth`: æœ€ä½³æ¨¡å‹
- `output/llm_gnn_fixed/training.log`: è®­ç»ƒæ—¥å¿—

### E. è”ç³»ä¿¡æ¯

**é¡¹ç›®**: LLMå¢å¼ºæ–°é—»æ¨èç³»ç»Ÿ
**æ—¥æœŸ**: 2025-11-29
**çŠ¶æ€**: è®­ç»ƒæˆåŠŸï¼Œæ¨¡å‹å¯ç”¨

---

**æŠ¥å‘Šç»“æŸ**
